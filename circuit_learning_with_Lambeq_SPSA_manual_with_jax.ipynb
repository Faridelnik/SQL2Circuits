{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09952d6-99a3-41a5-865f-2170e9ee0247",
   "metadata": {},
   "source": [
    "# Circuit learning module: Lambeq manually with SPSA and JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb811fe4-d729-4a44-838d-9c02db4a644d",
   "metadata": {},
   "source": [
    "This module performs the optimization of the parametrized circuit manually compared to Lambeq's automatic QuantumTrainer class. I created this because I wanted to have more control over the optimization process and debug it better. The code is based on the workflow presented in https://github.com/CQCL/Quanthoven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a5783f-901a-4bfe-a622-4003b345fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "from jax import numpy as np\n",
    "from sympy import default_sort_key\n",
    "import numpy\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "from jax import jit\n",
    "from noisyopt import minimizeSPSA, minimizeCompass\n",
    "\n",
    "from discopy.quantum import Circuit\n",
    "from discopy.tensor import Tensor\n",
    "from discopy.utils import loads\n",
    "#from pytket.extensions.qiskit import AerBackend\n",
    "#from pytket.extensions.qulacs import QulacsBackend\n",
    "#from pytket.extensions.cirq import CirqStateSampleBackend\n",
    "backend = None\n",
    "\n",
    "from utils import get_symbols, construct_data_and_labels, select_circuits, read_diagrams, create_labeled_classes, bin_class_loss, multi_class_loss, bin_class_acc, multi_class_acc, visualize_result_noisyopt\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "this_folder = os.path.abspath(os.getcwd())\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "#os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
    "\n",
    "EPOCHS = 500\n",
    "SEED = 0\n",
    "\n",
    "# This avoids TracerArrayConversionError from jax\n",
    "Tensor.np = np\n",
    "\n",
    "rng = numpy.random.default_rng(SEED)\n",
    "numpy.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238fca40-5d8d-4ace-a65f-693885a8caf8",
   "metadata": {},
   "source": [
    "## Read circuit data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6f00d7-b573-4c5d-a0d6-d5eafc54745e",
   "metadata": {},
   "source": [
    "We read the circuits from the pickled files. Select if we perform binary classification or multi-class classification. Give number of qubits to create classes:\n",
    "- 1 qubits -> 2^1 = 2 classes i.e. binary classification\n",
    "- 2 qubits -> 2^2 = 4 classes\n",
    "- ...\n",
    "- 5 qubits -> 2^5 = 32 classes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01bac1f9-3bc0-4a25-8f4f-baa2b9c581e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select workload\n",
    "workload = \"execution_time\"\n",
    "#workload = \"cardinality\"\n",
    "\n",
    "# Select workload size\n",
    "#workload_size = \"small\"\n",
    "#workload_size = \"medium\"\n",
    "#workload_size = \"large\"\n",
    "workload_size = \"main\"\n",
    "\n",
    "classification = 1\n",
    "layers = 1\n",
    "single_qubit_params = 3\n",
    "n_wire_count = 1\n",
    "\n",
    "loss = multi_class_loss\n",
    "acc = multi_class_acc\n",
    "\n",
    "if classification == 1:\n",
    "    loss = bin_class_loss\n",
    "    acc = bin_class_acc\n",
    "\n",
    "# Access the selected circuits\n",
    "path_name = this_folder + \"//simplified-JOB-diagrams//\" + workload + \"//\" + workload_size + \"//circuits//\" + str(classification) + \"//\" + str(layers) + \"_layer//\" + str(single_qubit_params) + \"_single_qubit_params//\" + str(n_wire_count) + \"_n_wire_count//\"\n",
    "\n",
    "training_circuits_paths = glob.glob(path_name + \"training//[0-9]*.p\")\n",
    "validation_circuits_paths = glob.glob(path_name + \"validation//[0-9]*.p\")\n",
    "test_circuits_paths = glob.glob(path_name + \"test//[0-9]*.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff0a551-444c-40a7-bf66-8e929bc49a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_circuits = read_diagrams(training_circuits_paths)\n",
    "validation_circuits = read_diagrams(validation_circuits_paths)\n",
    "test_circuits = read_diagrams(test_circuits_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da3a7c-6a93-44e9-9640-7e9ead005cb2",
   "metadata": {},
   "source": [
    "## Read training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "774e9f52-69c9-4e1c-8a70-76e1198d12f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data, validation_data = None, None, None\n",
    "data_path = this_folder + \"//data//\" + workload + \"//\" + workload_size + \"//\"\n",
    "\n",
    "with open(data_path + \"training_data.json\", \"r\") as inputfile:\n",
    "    training_data = json.load(inputfile)['training_data']\n",
    "with open(data_path + \"test_data.json\", \"r\") as inputfile:\n",
    "    test_data = json.load(inputfile)['test_data']\n",
    "with open(data_path + \"validation_data.json\", \"r\") as inputfile:\n",
    "    validation_data = json.load(inputfile)['validation_data']\n",
    "\n",
    "training_data_labels = create_labeled_classes(training_data, classification, workload)\n",
    "test_data_labels = create_labeled_classes(test_data, classification, workload)\n",
    "validation_data_labels = create_labeled_classes(validation_data, classification, workload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cad830-e6c3-4c29-9ce6-0d6bec2794bc",
   "metadata": {},
   "source": [
    "## Lambeq optimizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be555e95-e7aa-4f1b-bf4d-29acd66056b0",
   "metadata": {},
   "source": [
    "#all_circuits = list(training_circuits.values()) + list(test_circuits.values())\n",
    "\n",
    "training_circuits_l = []\n",
    "test_circuits_l = []\n",
    "validation_circuits_l = []\n",
    "\n",
    "training_data_labels_l = []\n",
    "test_data_labels_l = []\n",
    "validation_data_labels_l = []\n",
    "\n",
    "# Organize circuits and labels in correct order into two lists which will be input for training the model\n",
    "for key in training_data_labels:\n",
    "    training_circuits_l.append(training_circuits[key])\n",
    "    training_data_labels_l.append(training_data_labels[key])\n",
    "\n",
    "for key in test_data_labels:\n",
    "    test_circuits_l.append(test_circuits[key])\n",
    "    test_data_labels_l.append(test_data_labels[key])\n",
    "    \n",
    "for key in validation_data_labels:\n",
    "    validation_circuits_l.append(validation_circuits[key])\n",
    "    validation_data_labels_l.append(validation_data_labels[key])\n",
    "\n",
    "all_circuits = training_circuits_l + test_circuits_l + validation_circuits_l\n",
    "\n",
    "train_syms = set([sym for circuit in training_circuits.values() for sym in circuit.free_symbols])\n",
    "test_syms = set([sym for circuit in test_circuits.values() for sym in circuit.free_symbols])\n",
    "val_syms = set([sym for circuit in validation_circuits.values() for sym in circuit.free_symbols])\n",
    "\n",
    "print(\"Test circuits need to share training circuits' parameters. The parameters that are not covered (should be empty set, set()): \", test_syms.difference(train_syms))\n",
    "print(\"Validation circuits need to share training circuits' parameters. The parameters that are not covered (should be empty set, set()): \", val_syms.difference(train_syms))\n",
    "\n",
    "print(\"Total number of circuits: \", len(all_circuits))\n",
    "print(\"Total number of variables: \", len(train_syms))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ecd5daca-fdba-44fe-961d-5b9b9c3c68cb",
   "metadata": {},
   "source": [
    "training_data_labels_l = np.array(training_data_labels_l)\n",
    "test_data_labels_l = np.array(test_data_labels_l)\n",
    "validation_data_labels_l = np.array(validation_data_labels_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572abfc1-cc81-4fdb-b8bb-6813fe658dd6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75f97a9a-c5fc-4d9b-920f-9eb0795ca018",
   "metadata": {},
   "source": [
    "parameters = sorted(\n",
    "    train_syms,\n",
    "    key=default_sort_key)\n",
    "\n",
    "rng = numpy.random.default_rng(SEED)\n",
    "init_params_spsa = np.array(rng.random(len(parameters)))\n",
    "numpy.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c65634-fa13-47b3-915c-83361d0e1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred_fn(circuits):\n",
    "    # In the case we want to use other backends. \n",
    "    # Currently does not work properly.\n",
    "    if backend:\n",
    "        compiled_circuits1 = backend.get_compiled_circuits([c.to_tk() for c in circuits])\n",
    "        circuits = [Circuit.from_tk(c) for c in compiled_circuits1]\n",
    "        \n",
    "    circuit_fns = [c.lambdify(*parameters) for c in circuits]\n",
    "    \n",
    "    def predict(params):\n",
    "        outputs = Circuit.eval(*(c(*params) for c in circuit_fns), backend = backend)\n",
    "        res = []\n",
    "        \n",
    "        for output in outputs:\n",
    "            predictions = np.abs(output.array) + 1e-9\n",
    "            ratio = predictions / predictions.sum()\n",
    "            res.append(ratio)\n",
    "            \n",
    "        return np.array(res)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cf7e7-fcbf-4bad-b8bc-4ff0e6b29294",
   "metadata": {},
   "source": [
    "## Loss function and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f376b4aa-0ec6-4037-80eb-935aa75a2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cost_fn(pred_fn, labels):\n",
    "    def cost_fn(params, **kwargs):\n",
    "        predictions = pred_fn(params)\n",
    "\n",
    "        cost = loss(predictions, labels) #-np.sum(labels * np.log(predictions)) / len(labels)  # binary cross-entropy loss\n",
    "        costs.append(cost)\n",
    "\n",
    "        accuracy = acc(predictions, labels) #np.sum(np.round(predictions) == labels) / len(labels) / 2  # half due to double-counting\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        return cost\n",
    "\n",
    "    costs, accuracies = [], []\n",
    "    return cost_fn, costs, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bb6805-facb-4ec9-9c38-8d1df42022b2",
   "metadata": {},
   "source": [
    "## Minimization with noisyopt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53bc5cb8-8960-46d9-a64e-02b96fb97acc",
   "metadata": {},
   "source": [
    "EPOCHS = 1000\n",
    "SEED = 0\n",
    "\n",
    "# This avoids TracerArrayConversionError from jax\n",
    "Tensor.np = np\n",
    "\n",
    "rng = numpy.random.default_rng(SEED)\n",
    "init_params_spsa = np.array(rng.random(len(parameters)))\n",
    "numpy.random.seed(SEED)\n",
    "\n",
    "train_cost_fn, train_costs, train_accs = make_cost_fn(train_pred_fn, training_data_labels_l)\n",
    "dev_cost_fn, dev_costs, dev_accs = make_cost_fn(dev_pred_fn, validation_data_labels_l)\n",
    "\n",
    "def callback_fn(xk):\n",
    "    valid_loss = dev_cost_fn(xk)\n",
    "    train_loss = round(train_costs[-1], 4)\n",
    "    train_acc = round(train_accs[-1], 4)\n",
    "    valid_acc = round(dev_accs[-1], 4)\n",
    "    iters = int(len(train_accs)/2)\n",
    "    if iters % 100 == 0:\n",
    "        print(\n",
    "                #f\"Params = {xk}, \"\n",
    "                f\"Epoch: {iters}   \",\n",
    "                f\"train/loss: {train_loss}   \",\n",
    "                f\"valid/loss: {round(valid_loss, 4)}   \",\n",
    "                f\"train/acc: {train_acc}   \",\n",
    "                f\"valid/acc: {valid_acc}\"\n",
    "            )\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc202031-11f7-4307-96de-8c642d65ce4d",
   "metadata": {},
   "source": [
    "#a_values = [100.0, 10.0, 1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "#c_values = [100.0, 10.0, 1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "\n",
    "#a_values = [0.001, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03]\n",
    "#c_values = [0.001, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03]\n",
    "\n",
    "run = 6\n",
    "#a_values = [0.0055, 0.0054, 0.0052, 0.005, 0.0048, 0.0045, 0.004]\n",
    "#c_values = [0.012, 0.016, 0.018, 0.02, 0.022, 0.025, 0.03]\n",
    "\n",
    "#a_values = [0.0053, 0.0054, 0.0055, 0.0056, 0.0057]\n",
    "#c_values = [0.017, 0.0175, 0.018, 0.0185, 0.019]\n",
    "\n",
    "a_values = [0.0052, 0.00525, 0.0053, 0.00535, 0.0054]\n",
    "c_values = [0.0183, 0.0184, 0.0185, 0.0186, 0.0187] \n",
    "\n",
    "for i, a_value in enumerate(a_values):\n",
    "    for j, c_value in enumerate(c_values):\n",
    "        train_cost_fn, train_costs, train_accs = make_cost_fn(train_pred_fn, training_data_labels_l)\n",
    "        dev_cost_fn, dev_costs, dev_accs = make_cost_fn(dev_pred_fn, validation_data_labels_l)\n",
    "        \n",
    "        result = minimizeSPSA(train_cost_fn, x0=init_params_spsa, a = a_value, c = c_value, niter=EPOCHS, callback=callback_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85b70bae-fe88-4852-a423-658641380e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(old_params, old_values, new_params):\n",
    "    new_values = list(numpy.array(rng.random(len(new_params))))\n",
    "    old_param_dict = {}\n",
    "    for p, v in zip(old_params, old_values):\n",
    "        old_param_dict[p] = v\n",
    "        \n",
    "    parameters = sorted(set(old_params + new_params), key=default_sort_key)\n",
    "    values = []\n",
    "    for p in parameters:\n",
    "        if p in old_param_dict:\n",
    "            values.append(old_param_dict[p])\n",
    "        else:\n",
    "            values.append(new_values.pop())\n",
    "            \n",
    "    return parameters, np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dd1d87d-047d-4feb-9dbc-c972bbabf741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parameters from file main_noisyopt_21_1_3\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 4000\n",
    "initial_number_of_circuits = 321\n",
    "syms = {}\n",
    "limit = False\n",
    "all_training_keys = list(training_circuits.keys())\n",
    "initial_circuit_keys = all_training_keys[:initial_number_of_circuits + 1]\n",
    "current_training_circuits = {}\n",
    "result_file = workload_size + \"_noisyopt_2\" + str(classification) + \"_\" + str(layers) + \"_\" + str(single_qubit_params)\n",
    "\n",
    "for k in initial_circuit_keys:\n",
    "    current_training_circuits[k] = training_circuits[k]\n",
    "    \n",
    "syms = get_symbols(current_training_circuits)\n",
    "parameters = sorted(syms, key=default_sort_key)\n",
    "if initial_number_of_circuits > 5 and os.path.exists(\"points//\" + result_file + \".npz\"):\n",
    "    with open(\"points//\" + result_file + \".npz\", \"rb\") as f:\n",
    "        print(\"Loading parameters from file \" + result_file)\n",
    "        npzfile = np.load(f)\n",
    "        init_params_spsa = npzfile['arr_0']\n",
    "else:\n",
    "    print(\"Initializing new parameters\")\n",
    "    init_params_spsa = np.array(rng.random(len(parameters)))\n",
    "result = None\n",
    "run = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d19610-d096-4de1-b1b9-63aef886e632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  0.712\n",
      "Number of training circuits: 318   Number of validation circuits: 113   Number of test circuits: 111   Number of parameters in model: 271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 200   train/loss: 0.2728   valid/loss: 0.4432   train/acc: 0.8774   valid/acc: 0.8142\n",
      "Epoch: 400   train/loss: 0.272   valid/loss: 0.4401   train/acc: 0.8962   valid/acc: 0.8142\n",
      "Epoch: 600   train/loss: 0.3064   valid/loss: 0.4379   train/acc: 0.8679   valid/acc: 0.8142\n",
      "Epoch: 800   train/loss: 0.2957   valid/loss: 0.4388   train/acc: 0.8616   valid/acc: 0.8142\n",
      "Epoch: 1000   train/loss: 0.2901   valid/loss: 0.4374   train/acc: 0.8899   valid/acc: 0.823\n",
      "Epoch: 1200   train/loss: 0.2685   valid/loss: 0.4379   train/acc: 0.8899   valid/acc: 0.8142\n",
      "Epoch: 1400   train/loss: 0.3016   valid/loss: 0.4379   train/acc: 0.8836   valid/acc: 0.8142\n",
      "Epoch: 1600   train/loss: 0.2704   valid/loss: 0.4374   train/acc: 0.8931   valid/acc: 0.8142\n",
      "Epoch: 1800   train/loss: 0.2685   valid/loss: 0.4397   train/acc: 0.9025   valid/acc: 0.8142\n",
      "Epoch: 2000   train/loss: 0.2987   valid/loss: 0.4393   train/acc: 0.8774   valid/acc: 0.8142\n",
      "Epoch: 2200   train/loss: 0.2821   valid/loss: 0.4394   train/acc: 0.8805   valid/acc: 0.8142\n",
      "Epoch: 2400   train/loss: 0.2767   valid/loss: 0.4367   train/acc: 0.8836   valid/acc: 0.823\n",
      "Epoch: 2600   train/loss: 0.2917   valid/loss: 0.4376   train/acc: 0.8679   valid/acc: 0.8142\n",
      "Epoch: 2800   train/loss: 0.2743   valid/loss: 0.4374   train/acc: 0.8899   valid/acc: 0.8142\n",
      "Epoch: 3000   train/loss: 0.2847   valid/loss: 0.4376   train/acc: 0.8774   valid/acc: 0.8142\n",
      "Epoch: 3200   train/loss: 0.2839   valid/loss: 0.4388   train/acc: 0.8648   valid/acc: 0.8142\n",
      "Epoch: 3400   train/loss: 0.2842   valid/loss: 0.4402   train/acc: 0.8774   valid/acc: 0.8142\n",
      "Epoch: 3600   train/loss: 0.293   valid/loss: 0.4408   train/acc: 0.8868   valid/acc: 0.8142\n",
      "Epoch: 3800   train/loss: 0.275   valid/loss: 0.4394   train/acc: 0.8899   valid/acc: 0.8142\n",
      "Epoch: 4000   train/loss: 0.2644   valid/loss: 0.4397   train/acc: 0.8931   valid/acc: 0.8142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7747748\n",
      "Progress:  0.714\n",
      "Number of training circuits: 319   Number of validation circuits: 113   Number of test circuits: 111   Number of parameters in model: 271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 200   train/loss: 0.3005   valid/loss: 0.4509   train/acc: 0.8715   valid/acc: 0.823\n",
      "Epoch: 400   train/loss: 0.2798   valid/loss: 0.4448   train/acc: 0.8809   valid/acc: 0.823\n",
      "Epoch: 600   train/loss: 0.2995   valid/loss: 0.4437   train/acc: 0.8809   valid/acc: 0.8142\n",
      "Epoch: 800   train/loss: 0.2674   valid/loss: 0.444   train/acc: 0.8997   valid/acc: 0.8142\n",
      "Epoch: 1000   train/loss: 0.2747   valid/loss: 0.4428   train/acc: 0.884   valid/acc: 0.823\n",
      "Epoch: 1200   train/loss: 0.2769   valid/loss: 0.4441   train/acc: 0.884   valid/acc: 0.8142\n",
      "Epoch: 1400   train/loss: 0.2657   valid/loss: 0.4424   train/acc: 0.9028   valid/acc: 0.8142\n",
      "Epoch: 1600   train/loss: 0.2865   valid/loss: 0.4421   train/acc: 0.8997   valid/acc: 0.8142\n",
      "Epoch: 1800   train/loss: 0.2755   valid/loss: 0.4403   train/acc: 0.8903   valid/acc: 0.8142\n",
      "Epoch: 2000   train/loss: 0.2977   valid/loss: 0.441   train/acc: 0.884   valid/acc: 0.8142\n",
      "Epoch: 2200   train/loss: 0.2822   valid/loss: 0.4418   train/acc: 0.8746   valid/acc: 0.8142\n",
      "Epoch: 2400   train/loss: 0.2686   valid/loss: 0.4413   train/acc: 0.9028   valid/acc: 0.8142\n",
      "Epoch: 2600   train/loss: 0.2708   valid/loss: 0.4421   train/acc: 0.906   valid/acc: 0.8142\n",
      "Epoch: 2800   train/loss: 0.2693   valid/loss: 0.443   train/acc: 0.8903   valid/acc: 0.8142\n",
      "Epoch: 3000   train/loss: 0.2732   valid/loss: 0.4438   train/acc: 0.8809   valid/acc: 0.8142\n",
      "Epoch: 3200   train/loss: 0.2835   valid/loss: 0.4445   train/acc: 0.8903   valid/acc: 0.8142\n",
      "Epoch: 3400   train/loss: 0.2897   valid/loss: 0.4455   train/acc: 0.8777   valid/acc: 0.8142\n",
      "Epoch: 3600   train/loss: 0.2753   valid/loss: 0.4449   train/acc: 0.8871   valid/acc: 0.8142\n",
      "Epoch: 3800   train/loss: 0.2669   valid/loss: 0.4457   train/acc: 0.8934   valid/acc: 0.8142\n",
      "Epoch: 4000   train/loss: 0.2803   valid/loss: 0.4436   train/acc: 0.884   valid/acc: 0.8142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7747748\n",
      "Progress:  0.717\n",
      "Number of training circuits: 320   Number of validation circuits: 113   Number of test circuits: 111   Number of parameters in model: 271\n"
     ]
    }
   ],
   "source": [
    "for i, key in enumerate(all_training_keys[initial_number_of_circuits:]):\n",
    "    print(\"Progress: \", round((i + initial_number_of_circuits)/len(all_training_keys), 3))\n",
    "    \n",
    "    if len(syms) == len(get_symbols(current_training_circuits)) and i > 0:\n",
    "        if i != len(all_training_keys[1:]):\n",
    "            current_training_circuits[key] = training_circuits[key]\n",
    "            new_parameters = sorted(get_symbols({key: training_circuits[key]}), key=default_sort_key)\n",
    "            if result:\n",
    "                parameters, init_params_spsa = initialize_parameters(parameters, result.x, new_parameters)\n",
    "                #continue\n",
    "            else:\n",
    "                syms = get_symbols(current_training_circuits)\n",
    "                parameters = sorted(syms, key=default_sort_key)\n",
    "                init_params_spsa = np.array(rng.random(len(parameters)))\n",
    "    else:\n",
    "        run += 1\n",
    "    \n",
    "    # Select those circuits from test and validation circuits which share the parameters with the current training circuits\n",
    "    current_validation_circuits = select_circuits(current_training_circuits, validation_circuits)\n",
    "    current_test_circuits = select_circuits(current_training_circuits, test_circuits)\n",
    "    \n",
    "    if len(current_validation_circuits) == 0 or len(current_test_circuits) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Create lists with circuits and their corresponding label\n",
    "    training_circuits_l, training_data_labels_l = construct_data_and_labels(current_training_circuits, training_data_labels)\n",
    "    validation_circuits_l, validation_data_labels_l = construct_data_and_labels(current_validation_circuits, validation_data_labels)\n",
    "    test_circuits_l, test_data_labels_l = construct_data_and_labels(current_test_circuits, test_data_labels)\n",
    "    \n",
    "    # Limit the number of validation and test circuits to 20% of number of the training circuits\n",
    "    if limit:\n",
    "        val_test_circ_size = ceil(len(current_training_circuits))\n",
    "        if len(current_validation_circuits) > val_test_circ_size:\n",
    "            validation_circuits_l = validation_circuits_l[:val_test_circ_size]\n",
    "            validation_data_labels_l = validation_data_labels_l[:val_test_circ_size]\n",
    "        if len(current_test_circuits) > val_test_circ_size:\n",
    "            test_circuits_l = test_circuits_l[:val_test_circ_size]\n",
    "            test_data_labels_l = test_data_labels_l[:val_test_circ_size]\n",
    "    \n",
    "    stats = f\"Number of training circuits: {len(training_circuits_l)}   \"\\\n",
    "        + f\"Number of validation circuits: {len(validation_circuits_l)}   \"\\\n",
    "        + f\"Number of test circuits: {len(test_circuits_l)}   \"\\\n",
    "        + f\"Number of parameters in model: {len(set([sym for circuit in training_circuits_l for sym in circuit.free_symbols]))}\"\n",
    "    \n",
    "    with open(\"results//\" + result_file + \".txt\", \"a\") as f:\n",
    "        f.write(stats + \"\\n\")\n",
    "    \n",
    "    print(stats)\n",
    "    \n",
    "    train_pred_fn = jit(make_pred_fn(training_circuits_l))\n",
    "    dev_pred_fn = jit(make_pred_fn(validation_circuits_l))\n",
    "    test_pred_fn = make_pred_fn(test_circuits_l)\n",
    "    \n",
    "    train_cost_fn, train_costs, train_accs = make_cost_fn(train_pred_fn, training_data_labels_l)\n",
    "    dev_cost_fn, dev_costs, dev_accs = make_cost_fn(dev_pred_fn, validation_data_labels_l)\n",
    "    \n",
    "    def callback_fn(xk):\n",
    "        #print(xk)\n",
    "        valid_loss = dev_cost_fn(xk)\n",
    "        train_loss = numpy.around(min(float(train_costs[-1]), float(train_costs[-2])), 4)\n",
    "        train_acc = numpy.around(min(float(train_accs[-1]), float(train_accs[-2])), 4)\n",
    "        valid_acc = numpy.around(float(dev_accs[-1]), 4)\n",
    "        iters = int(len(train_accs)/2)\n",
    "        if iters % 200 == 0:\n",
    "            info = f\"Epoch: {iters}   \"\\\n",
    "            + f\"train/loss: {train_loss}   \"\\\n",
    "            + f\"valid/loss: {numpy.around(float(valid_loss), 4)}   \"\\\n",
    "            + f\"train/acc: {train_acc}   \"\\\n",
    "            + f\"valid/acc: {valid_acc}\"\n",
    "        \n",
    "            with open(\"results//\" + result_file + \".txt\", \"a\") as f:\n",
    "                f.write(info + \"\\n\")\n",
    "                \n",
    "            print(info, file=sys.stderr)\n",
    "        return valid_loss\n",
    "    \n",
    "    a_value = 0.0053\n",
    "    c_value = 0.0185\n",
    "            \n",
    "    train_cost_fn, train_costs, train_accs = make_cost_fn(train_pred_fn, training_data_labels_l)\n",
    "    dev_cost_fn, dev_costs, dev_accs = make_cost_fn(dev_pred_fn, validation_data_labels_l)\n",
    "\n",
    "    result = minimizeSPSA(train_cost_fn, x0=init_params_spsa, a = a_value, c = c_value, niter=EPOCHS, callback=callback_fn)\n",
    "    #result = minimizeCompass(train_cost_fn, x0=init_params_spsa, redfactor=2.0, deltainit=1.0, deltatol=0.001, feps=1e-15, errorcontrol=True, funcNinit=30, funcmultfactor=2.0, paired=True, alpha=0.05, callback=callback_fn)\n",
    "\n",
    "    figure_path = this_folder + \"//results//\" + result_file + \".png\"\n",
    "    visualize_result_noisyopt(result, make_cost_fn, test_pred_fn, test_data_labels_l, train_costs, train_accs, dev_costs, dev_accs, figure_path, result_file)\n",
    "    \n",
    "    run += 1\n",
    "    #EPOCHS += 100\n",
    "    syms = get_symbols(current_training_circuits)\n",
    "    \n",
    "    # Extend for the next optimization round\n",
    "    current_training_circuits[key] = training_circuits[key]\n",
    "    new_parameters = sorted(get_symbols({key: training_circuits[key]}), key=default_sort_key)\n",
    "    parameters, init_params_spsa = initialize_parameters(parameters, result.x, new_parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
