{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09952d6-99a3-41a5-865f-2170e9ee0247",
   "metadata": {},
   "source": [
    "# Circuit learning module: Lambeq manually with SPSA and JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb811fe4-d729-4a44-838d-9c02db4a644d",
   "metadata": {},
   "source": [
    "This module performs the optimization of the parametrized circuit manually compared to Lambeq's automatic QuantumTrainer class. I created this because I wanted to have more control over the optimization process and debug it better. The code is based on the workflow presented in https://github.com/CQCL/Quanthoven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a5783f-901a-4bfe-a622-4003b345fb1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jaxlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ceil\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_sort_key\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\__init__.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _os\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# flake8: noqa: F401\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m   ad,  \u001b[38;5;66;03m# TODO(phawkins): update users to avoid this.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m   argnums_partial,  \u001b[38;5;66;03m# TODO(phawkins): update Haiku to not use this.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m   xla_computation,\n\u001b[0;32m     88\u001b[0m )\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmaps\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m soft_pmap\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\config.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbool_env\u001b[39m(varname: \u001b[38;5;28mstr\u001b[39m, default: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m     22\u001b[0m   \u001b[38;5;124;03m\"\"\"Read an environment variable and interpret it as a boolean.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m  True values are (case insensitive): 'y', 'yes', 't', 'true', 'on', and '1';\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m  Raises: ValueError if the environment variable is anything else.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\lib\\__init__.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2018 Google LLC\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# This module is largely a wrapper around `jaxlib` that performs version\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# checking on import.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     19\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda_prng\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcusolver\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrocsolver\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjaxlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlapack\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpocketfft\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytree\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtpu_client\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxla_client\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     21\u001b[0m ]\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjaxlib\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Must be kept in sync with the jaxlib version in build/test-requirements.txt\u001b[39;00m\n\u001b[0;32m     26\u001b[0m _minimum_jaxlib_version \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jaxlib'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "from jax import numpy as np\n",
    "from sympy import default_sort_key\n",
    "import numpy\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "from jax import jit\n",
    "from noisyopt import minimizeSPSA, minimizeCompass\n",
    "\n",
    "from discopy.quantum import Circuit\n",
    "from discopy.tensor import Tensor\n",
    "from discopy.utils import loads\n",
    "#from pytket.extensions.qiskit import AerBackend\n",
    "#from pytket.extensions.qulacs import QulacsBackend\n",
    "#from pytket.extensions.cirq import CirqStateSampleBackend\n",
    "backend = None\n",
    "\n",
    "from utils import *\n",
    "#from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "this_folder = os.path.abspath(os.getcwd())\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "#os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "# This avoids TracerArrayConversionError from jax\n",
    "Tensor.np = np\n",
    "\n",
    "rng = numpy.random.default_rng(SEED)\n",
    "numpy.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238fca40-5d8d-4ace-a65f-693885a8caf8",
   "metadata": {},
   "source": [
    "## Read circuit data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6f00d7-b573-4c5d-a0d6-d5eafc54745e",
   "metadata": {},
   "source": [
    "We read the circuits from the pickled files. Select if we perform binary classification or multi-class classification. Give number of qubits to create classes:\n",
    "- 1 qubits -> 2^1 = 2 classes i.e. binary classification\n",
    "- 2 qubits -> 2^2 = 4 classes\n",
    "- ...\n",
    "- 5 qubits -> 2^5 = 32 classes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bac1f9-3bc0-4a25-8f4f-baa2b9c581e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select workload\n",
    "#workload = \"execution_time\"\n",
    "workload = \"cardinality\"\n",
    "\n",
    "# Select workload size\n",
    "#workload_size = \"small\"\n",
    "#workload_size = \"medium\"\n",
    "#workload_size = \"large\"\n",
    "workload_size = \"main\"\n",
    "\n",
    "classification = 2\n",
    "layers = 1\n",
    "single_qubit_params = 3\n",
    "n_wire_count = 1\n",
    "\n",
    "loss = multi_class_loss\n",
    "acc = multi_class_acc\n",
    "\n",
    "if classification == 1:\n",
    "    loss = bin_class_loss\n",
    "    acc = bin_class_acc\n",
    "\n",
    "# Access the selected circuits\n",
    "path_name = this_folder + \"//simplified-JOB-diagrams//\" + workload + \"//\" + workload_size + \"//circuits//\" + str(classification) + \"//\" + str(layers) + \"_layer//\" + str(single_qubit_params) + \"_single_qubit_params//\" + str(n_wire_count) + \"_n_wire_count//\"\n",
    "\n",
    "training_circuits_paths = glob.glob(path_name + \"training//[0-9]*.p\")\n",
    "validation_circuits_paths = glob.glob(path_name + \"validation//[0-9]*.p\")\n",
    "test_circuits_paths = glob.glob(path_name + \"test//[0-9]*.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff0a551-444c-40a7-bf66-8e929bc49a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_circuits = read_diagrams(training_circuits_paths)\n",
    "validation_circuits = read_diagrams(validation_circuits_paths)\n",
    "test_circuits = read_diagrams(test_circuits_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da3a7c-6a93-44e9-9640-7e9ead005cb2",
   "metadata": {},
   "source": [
    "## Read training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e9f52-69c9-4e1c-8a70-76e1198d12f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data, validation_data = None, None, None\n",
    "data_path = this_folder + \"//data//\" + workload + \"//\" + workload_size + \"//\"\n",
    "\n",
    "with open(data_path + \"training_data.json\", \"r\") as inputfile:\n",
    "    training_data = json.load(inputfile)['training_data']\n",
    "with open(data_path + \"test_data.json\", \"r\") as inputfile:\n",
    "    test_data = json.load(inputfile)['test_data']\n",
    "with open(data_path + \"validation_data.json\", \"r\") as inputfile:\n",
    "    validation_data = json.load(inputfile)['validation_data']\n",
    "\n",
    "training_data_labels = create_labeled_classes(training_data, classification, workload)\n",
    "test_data_labels = create_labeled_classes(test_data, classification, workload)\n",
    "validation_data_labels = create_labeled_classes(validation_data, classification, workload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572abfc1-cc81-4fdb-b8bb-6813fe658dd6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c65634-fa13-47b3-915c-83361d0e1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred_fn(circuits):\n",
    "    # In the case we want to use other backends. \n",
    "    # Currently does not work properly.\n",
    "    if backend:\n",
    "        compiled_circuits1 = backend.get_compiled_circuits([c.to_tk() for c in circuits])\n",
    "        circuits = [Circuit.from_tk(c) for c in compiled_circuits1]\n",
    "        \n",
    "    circuit_fns = [c.lambdify(*parameters) for c in circuits]\n",
    "    \n",
    "    def predict(params):\n",
    "        outputs = Circuit.eval(*(c(*params) for c in circuit_fns), backend = backend)\n",
    "        res = []\n",
    "        \n",
    "        for output in outputs:\n",
    "            predictions = np.abs(output.array) + 1e-9\n",
    "            ratio = predictions / predictions.sum()\n",
    "            res.append(ratio)\n",
    "            \n",
    "        return np.array(res)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cf7e7-fcbf-4bad-b8bc-4ff0e6b29294",
   "metadata": {},
   "source": [
    "## Loss function and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376b4aa-0ec6-4037-80eb-935aa75a2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cost_fn(pred_fn, labels):\n",
    "    def cost_fn(params, **kwargs):\n",
    "        predictions = pred_fn(params)\n",
    "        #print(predictions, labels)\n",
    "        \n",
    "        cost = loss(predictions, labels) #-np.sum(labels * np.log(predictions)) / len(labels)  # binary cross-entropy loss\n",
    "        costs.append(cost)\n",
    "\n",
    "        accuracy = acc(predictions, labels) #np.sum(np.round(predictions) == labels) / len(labels) / 2  # half due to double-counting\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        return cost\n",
    "\n",
    "    costs, accuracies = [], []\n",
    "    return cost_fn, costs, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bb6805-facb-4ec9-9c38-8d1df42022b2",
   "metadata": {},
   "source": [
    "## Minimization with noisyopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b70bae-fe88-4852-a423-658641380e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(old_params, old_values, new_params):\n",
    "    new_values = list(numpy.array(rng.random(len(new_params))))\n",
    "    old_param_dict = {}\n",
    "    for p, v in zip(old_params, old_values):\n",
    "        old_param_dict[p] = v\n",
    "        \n",
    "    parameters = sorted(set(old_params + new_params), key=default_sort_key)\n",
    "    values = []\n",
    "    for p in parameters:\n",
    "        if p in old_param_dict:\n",
    "            values.append(old_param_dict[p])\n",
    "        else:\n",
    "            values.append(new_values.pop())\n",
    "            \n",
    "    return parameters, np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd1d87d-047d-4feb-9dbc-c972bbabf741",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "initial_number_of_circuits = 5\n",
    "syms = {}\n",
    "limit = False\n",
    "all_training_keys = list(training_circuits.keys())\n",
    "initial_circuit_keys = all_training_keys[:initial_number_of_circuits + 1]\n",
    "current_training_circuits = {}\n",
    "result_file = workload + \"_\" + workload_size + \"_noisyopt_2_\" + str(classification) + \"_\" + str(layers) + \"_\" + str(single_qubit_params)\n",
    "\n",
    "for k in initial_circuit_keys:\n",
    "    current_training_circuits[k] = training_circuits[k]\n",
    "    \n",
    "syms = get_symbols(current_training_circuits)\n",
    "parameters = sorted(syms, key=default_sort_key)\n",
    "if initial_number_of_circuits > 20 and os.path.exists(\"points//\" + result_file + \".npz\"):\n",
    "    with open(\"points//\" + result_file + \".npz\", \"rb\") as f:\n",
    "        print(\"Loading parameters from file \" + result_file)\n",
    "        npzfile = np.load(f)\n",
    "        init_params_spsa = npzfile['arr_0']\n",
    "else:\n",
    "    print(\"Initializing new parameters\")\n",
    "    init_params_spsa = np.array(rng.random(len(parameters)))\n",
    "result = None\n",
    "run = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d19610-d096-4de1-b1b9-63aef886e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = None\n",
    "\n",
    "for i, key in enumerate(all_training_keys[initial_number_of_circuits:]):\n",
    "    print(\"Progress: \", round((i + initial_number_of_circuits)/len(all_training_keys), 3))\n",
    "    \n",
    "    if len(syms) == len(get_symbols(current_training_circuits)) and i > 0:\n",
    "        if i != len(all_training_keys[1:]):\n",
    "            current_training_circuits[key] = training_circuits[key]\n",
    "            new_parameters = sorted(get_symbols({key: training_circuits[key]}), key=default_sort_key)\n",
    "            if result:\n",
    "                parameters, init_params_spsa = initialize_parameters(parameters, result.x, new_parameters)\n",
    "                #continue\n",
    "            else:\n",
    "                syms = get_symbols(current_training_circuits)\n",
    "                parameters = sorted(syms, key=default_sort_key)\n",
    "                init_params_spsa = np.array(rng.random(len(parameters)))\n",
    "    else:\n",
    "        run += 1\n",
    "    \n",
    "    # Select those circuits from test and validation circuits which share the parameters with the current training circuits\n",
    "    current_validation_circuits = select_circuits(current_training_circuits, validation_circuits)\n",
    "    current_test_circuits = select_circuits(current_training_circuits, test_circuits)\n",
    "    \n",
    "    if len(current_validation_circuits) == 0 or len(current_test_circuits) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Create lists with circuits and their corresponding label\n",
    "    training_circuits_l, training_data_labels_l = construct_data_and_labels(current_training_circuits, training_data_labels)\n",
    "    validation_circuits_l, validation_data_labels_l = construct_data_and_labels(current_validation_circuits, validation_data_labels)\n",
    "    test_circuits_l, test_data_labels_l = construct_data_and_labels(current_test_circuits, test_data_labels)\n",
    "    \n",
    "    # Limit the number of validation and test circuits to 20% of number of the training circuits\n",
    "    if limit:\n",
    "        val_test_circ_size = ceil(len(current_training_circuits))\n",
    "        if len(current_validation_circuits) > val_test_circ_size:\n",
    "            validation_circuits_l = validation_circuits_l[:val_test_circ_size]\n",
    "            validation_data_labels_l = validation_data_labels_l[:val_test_circ_size]\n",
    "        if len(current_test_circuits) > val_test_circ_size:\n",
    "            test_circuits_l = test_circuits_l[:val_test_circ_size]\n",
    "            test_data_labels_l = test_data_labels_l[:val_test_circ_size]\n",
    "    \n",
    "    stats = f\"Number of training circuits: {len(training_circuits_l)}   \"\\\n",
    "        + f\"Number of validation circuits: {len(validation_circuits_l)}   \"\\\n",
    "        + f\"Number of test circuits: {len(test_circuits_l)}   \"\\\n",
    "        + f\"Number of parameters in model: {len(set([sym for circuit in training_circuits_l for sym in circuit.free_symbols]))}\"\n",
    "    \n",
    "    with open(\"results//\" + result_file + \".txt\", \"a\") as f:\n",
    "        f.write(stats + \"\\n\")\n",
    "    \n",
    "    print(stats)\n",
    "    \n",
    "    optimization_interval = 3\n",
    "    \n",
    "    if result == None or run % optimization_interval == 0:\n",
    "    \n",
    "        train_pred_fn = jit(make_pred_fn(training_circuits_l))\n",
    "        dev_pred_fn = jit(make_pred_fn(validation_circuits_l))\n",
    "        test_pred_fn = make_pred_fn(test_circuits_l)\n",
    "\n",
    "        train_cost_fn, train_costs, train_accs = make_cost_fn(train_pred_fn, training_data_labels_l)\n",
    "        dev_cost_fn, dev_costs, dev_accs = make_cost_fn(dev_pred_fn, validation_data_labels_l)\n",
    "\n",
    "        def callback_fn(xk):\n",
    "            #print(xk)\n",
    "            valid_loss = dev_cost_fn(xk)\n",
    "            train_loss = numpy.around(min(float(train_costs[-1]), float(train_costs[-2])), 4)\n",
    "            train_acc = numpy.around(min(float(train_accs[-1]), float(train_accs[-2])), 4)\n",
    "            valid_acc = numpy.around(float(dev_accs[-1]), 4)\n",
    "            iters = int(len(train_accs)/2)\n",
    "            if iters % 200 == 0:\n",
    "                info = f\"Epoch: {iters}   \"\\\n",
    "                + f\"train/loss: {train_loss}   \"\\\n",
    "                + f\"valid/loss: {numpy.around(float(valid_loss), 4)}   \"\\\n",
    "                + f\"train/acc: {train_acc}   \"\\\n",
    "                + f\"valid/acc: {valid_acc}\"\n",
    "\n",
    "                with open(\"results//\" + result_file + \".txt\", \"a\") as f:\n",
    "                    f.write(info + \"\\n\")\n",
    "\n",
    "                print(info, file=sys.stderr)\n",
    "            return valid_loss\n",
    "\n",
    "        #a_value = 0.0053\n",
    "        #c_value = 0.0185\n",
    "\n",
    "        # Good\n",
    "        #a_value = 0.053\n",
    "        #c_value = 0.00185\n",
    "\n",
    "        a_value = 0.0053\n",
    "        c_value = 0.00185\n",
    "\n",
    "        train_cost_fn, train_costs, train_accs = make_cost_fn(train_pred_fn, training_data_labels_l)\n",
    "        dev_cost_fn, dev_costs, dev_accs = make_cost_fn(dev_pred_fn, validation_data_labels_l)\n",
    "\n",
    "        result = minimizeSPSA(train_cost_fn, x0=init_params_spsa, a = a_value, c = c_value, niter=EPOCHS, callback=callback_fn)\n",
    "        #result = minimizeCompass(train_cost_fn, x0=init_params_spsa, redfactor=2.0, deltainit=1.0, deltatol=0.001, feps=1e-15, errorcontrol=True, funcNinit=30, funcmultfactor=2.0, paired=True, alpha=0.05, callback=callback_fn)\n",
    "\n",
    "        figure_path = this_folder + \"//results//\" + result_file + \".png\"\n",
    "        visualize_result_noisyopt(result, make_cost_fn, test_pred_fn, test_data_labels_l, train_costs, train_accs, dev_costs, dev_accs, figure_path, result_file)\n",
    "    \n",
    "    run += 1\n",
    "    #EPOCHS += 100\n",
    "    syms = get_symbols(current_training_circuits)\n",
    "    \n",
    "    # Extend for the next optimization round\n",
    "    current_training_circuits[key] = training_circuits[key]\n",
    "    new_parameters = sorted(get_symbols({key: training_circuits[key]}), key=default_sort_key)\n",
    "    parameters, init_params_spsa = initialize_parameters(parameters, result.x, new_parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
