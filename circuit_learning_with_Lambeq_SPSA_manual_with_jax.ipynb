{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09952d6-99a3-41a5-865f-2170e9ee0247",
   "metadata": {},
   "source": [
    "# Circuit learning module: Lambeq manually with SPSA and JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb811fe4-d729-4a44-838d-9c02db4a644d",
   "metadata": {},
   "source": [
    "This module performs the optimization of the parametrized circuit manually compared to Lambeq's automatic QuantumTrainer class. I created this because I wanted to have more control over the optimization process and debug it better. The code is based on the workflow presented in https://github.com/CQCL/Quanthoven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a5783f-901a-4bfe-a622-4003b345fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from jax import numpy as np\n",
    "from sympy import default_sort_key\n",
    "#import numpy as np\n",
    "import numpy\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from discopy.utils import loads\n",
    "from pytket.extensions.qiskit import AerBackend\n",
    "from pytket.extensions.qulacs import QulacsBackend\n",
    "from pytket.extensions.cirq import CirqStateSampleBackend\n",
    "\n",
    "backend = None\n",
    "\n",
    "from discopy.quantum import Circuit\n",
    "from discopy.tensor import Tensor\n",
    "\n",
    "from utils import read_diagrams, create_labeled_classes, loss, acc\n",
    "\n",
    "import jax\n",
    "from jax import jit\n",
    "from noisyopt import minimizeSPSA\n",
    "\n",
    "this_folder = os.path.abspath(os.getcwd())\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "#os.environ[\"JAX_PLATFORMS\"] = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238fca40-5d8d-4ace-a65f-693885a8caf8",
   "metadata": {},
   "source": [
    "## Read circuit data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6f00d7-b573-4c5d-a0d6-d5eafc54745e",
   "metadata": {},
   "source": [
    "We read the circuits from the pickled files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01bac1f9-3bc0-4a25-8f4f-baa2b9c581e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select workload\n",
    "#workload = \"small\"\n",
    "workload = \"medium\"\n",
    "#workload = \"large\"\n",
    "\n",
    "# Select if we perform binary classification or multi-class classification\n",
    "# Give number of qubits to create classes:\n",
    "# 1 -> 2^1 = 2 classes i.e. binary classification\n",
    "# 2 -> 2^2 = 4 classes\n",
    "# ...\n",
    "# 5 -> 2^5 = 32 classes, etc.\n",
    "\n",
    "classification = 2\n",
    "\n",
    "# Access the selected circuits\n",
    "path_name = this_folder + \"//simplified-JOB-diagrams//\" + workload + \"//circuits//\" + str(classification) + \"//\"\n",
    "\n",
    "training_circuits_paths = glob.glob(path_name + \"training//[0-9]*.p\")\n",
    "validation_circuits_paths = glob.glob(path_name + \"validation//[0-9]*.p\")\n",
    "test_circuits_paths = glob.glob(path_name + \"test//[0-9]*.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff0a551-444c-40a7-bf66-8e929bc49a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_circuits = read_diagrams(training_circuits_paths)\n",
    "validation_circuits = read_diagrams(validation_circuits_paths)\n",
    "test_circuits = read_diagrams(test_circuits_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da3a7c-6a93-44e9-9640-7e9ead005cb2",
   "metadata": {},
   "source": [
    "## Read training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "774e9f52-69c9-4e1c-8a70-76e1198d12f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data, validation_data = None, None, None\n",
    "data_path = this_folder + \"//data//\" + workload + \"//\"\n",
    "\n",
    "with open(data_path + \"training_data.json\", \"r\") as inputfile:\n",
    "    training_data = json.load(inputfile)['training_data']\n",
    "with open(data_path + \"test_data.json\", \"r\") as inputfile:\n",
    "    test_data = json.load(inputfile)['test_data']\n",
    "with open(data_path + \"validation_data.json\", \"r\") as inputfile:\n",
    "    validation_data = json.load(inputfile)['validation_data']\n",
    "\n",
    "training_data_labels = create_labeled_classes(training_data, classification)\n",
    "test_data_labels = create_labeled_classes(test_data, classification)\n",
    "validation_data_labels = create_labeled_classes(validation_data, classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cad830-e6c3-4c29-9ce6-0d6bec2794bc",
   "metadata": {},
   "source": [
    "## Lambeq optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce559dbf-2b4b-4853-82ed-58832aa11e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test circuits need to share training circuits' parameters. The parameters that are not covered (should be empty set, set()):  set()\n",
      "Validation circuits need to share training circuits' parameters. The parameters that are not covered (should be empty set, set()):  set()\n",
      "Total number of circuits:  121\n",
      "Total number of variables:  77\n"
     ]
    }
   ],
   "source": [
    "#all_circuits = list(training_circuits.values()) + list(test_circuits.values())\n",
    "\n",
    "training_circuits_l = []\n",
    "test_circuits_l = []\n",
    "validation_circuits_l = []\n",
    "\n",
    "training_data_labels_l = []\n",
    "test_data_labels_l = []\n",
    "validation_data_labels_l = []\n",
    "\n",
    "# Organize circuits and labels in correct order into two lists which will be input for training the model\n",
    "for key in training_data_labels:\n",
    "    training_circuits_l.append(training_circuits[key])\n",
    "    training_data_labels_l.append(training_data_labels[key])\n",
    "\n",
    "for key in test_data_labels:\n",
    "    test_circuits_l.append(test_circuits[key])\n",
    "    test_data_labels_l.append(test_data_labels[key])\n",
    "    \n",
    "for key in validation_data_labels:\n",
    "    validation_circuits_l.append(validation_circuits[key])\n",
    "    validation_data_labels_l.append(validation_data_labels[key])\n",
    "\n",
    "all_circuits = training_circuits_l + test_circuits_l + validation_circuits_l\n",
    "\n",
    "train_syms = set([sym for circuit in training_circuits.values() for sym in circuit.free_symbols])\n",
    "test_syms = set([sym for circuit in test_circuits.values() for sym in circuit.free_symbols])\n",
    "val_syms = set([sym for circuit in validation_circuits.values() for sym in circuit.free_symbols])\n",
    "\n",
    "print(\"Test circuits need to share training circuits' parameters. The parameters that are not covered (should be empty set, set()): \", test_syms.difference(train_syms))\n",
    "print(\"Validation circuits need to share training circuits' parameters. The parameters that are not covered (should be empty set, set()): \", val_syms.difference(train_syms))\n",
    "\n",
    "print(\"Total number of circuits: \", len(all_circuits))\n",
    "print(\"Total number of variables: \", len(train_syms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c84f333-8670-492f-b85f-20467a2f7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_labels_l = np.array(training_data_labels_l)\n",
    "test_data_labels_l = np.array(test_data_labels_l)\n",
    "validation_data_labels_l = np.array(validation_data_labels_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572abfc1-cc81-4fdb-b8bb-6813fe658dd6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e772e8c8-b747-4c19-8eef-7cc038194d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = sorted(\n",
    "    train_syms,\n",
    "    key=default_sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04c65634-fa13-47b3-915c-83361d0e1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred_fn(circuits):\n",
    "    if backend:\n",
    "        compiled_circuits1 = backend.get_compiled_circuits([c.to_tk() for c in circuits])\n",
    "        circuits = [Circuit.from_tk(c) for c in compiled_circuits1]\n",
    "        \n",
    "    circuit_fns = [c.lambdify(*parameters) for c in circuits]\n",
    "    \n",
    "    def predict(params):\n",
    "        outputs = Circuit.eval(*(c(*params) for c in circuit_fns), backend = backend)\n",
    "        res = []\n",
    "        \n",
    "        for output in outputs:\n",
    "            predictions = np.abs(output.array) + 1e-9\n",
    "            ratio = predictions / predictions.sum()\n",
    "            res.append(ratio)\n",
    "            \n",
    "        return np.array(res)\n",
    "    return predict\n",
    "\n",
    "train_pred_fn = jit(make_pred_fn(training_circuits_l))\n",
    "dev_pred_fn = jit(make_pred_fn(validation_circuits_l))\n",
    "test_pred_fn = make_pred_fn(test_circuits_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cf7e7-fcbf-4bad-b8bc-4ff0e6b29294",
   "metadata": {},
   "source": [
    "## Loss function and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f376b4aa-0ec6-4037-80eb-935aa75a2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cost_fn(pred_fn, labels):\n",
    "    def cost_fn(params, **kwargs):\n",
    "        predictions = pred_fn(params)\n",
    "\n",
    "        cost = loss(predictions, labels) #-np.sum(labels * np.log(predictions)) / len(labels)  # binary cross-entropy loss\n",
    "        costs.append(cost)\n",
    "\n",
    "        accuracy = acc(predictions, labels) #np.sum(np.round(predictions) == labels) / len(labels) / 2  # half due to double-counting\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        return cost\n",
    "\n",
    "    costs, accuracies = [], []\n",
    "    return cost_fn, costs, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bb6805-facb-4ec9-9c38-8d1df42022b2",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575f6418-781f-4d16-bf0f-7dc9dacdf0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "SEED = 0\n",
    "\n",
    "# This avoids TracerArrayConversionError from jax\n",
    "Tensor.np = np\n",
    "\n",
    "import numpy\n",
    "\n",
    "rng = numpy.random.default_rng(SEED)\n",
    "init_params_spsa = np.array(rng.random(len(parameters)))\n",
    "numpy.random.seed(SEED)\n",
    "\n",
    "#print(jax.make_jaxpr(train_pred_fn)(init_params_spsa))\n",
    "\n",
    "train_cost_fn, train_costs, train_accs = make_cost_fn(train_pred_fn, training_data_labels_l)\n",
    "dev_cost_fn, dev_costs, dev_accs = make_cost_fn(dev_pred_fn, validation_data_labels_l)\n",
    "\n",
    "def callback_fn(xk):\n",
    "    #cost_val = train_cost_fn(xk, costs = cost_store_spsa, accuracies = acc_store_spsa)\n",
    "    cost_val = train_costs[-1]\n",
    "    acc_val = train_accs[-1]\n",
    "    iters = len(train_accs)/2\n",
    "    if iters % 100 == 0:\n",
    "        print(\n",
    "                #f\"Params = {xk}, \"\n",
    "                f\"Iteration = {iters}, \"\n",
    "                f\"Cost = {cost_val}\",\n",
    "                f\"Accuracy on training data = {acc_val}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dbc0e6-4261-498f-92fc-f71e28a8aab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "     fun: 0.36933543533086777\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([0.6369617 , 0.26978672, 0.04097353, ..., 0.4998958 ,\n",
      "             0.42522863, 0.62021345], dtype=float32)\n",
      "Test accuracy: 0.3625\n",
      "0.1\n",
      "     fun: 0.3409697938710451\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([0.69526476, 0.2722515 , 0.06089482, ..., 0.50325686,\n",
      "             0.4839961 , 0.5586358 ], dtype=float32)\n",
      "Test accuracy: 0.375\n",
      "0.2\n",
      "     fun: 0.3099510231986642\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([0.7304295 , 0.2806479 , 0.02302931, ..., 0.57179695,\n",
      "             0.45437345, 0.5036008 ], dtype=float32)\n",
      "Test accuracy: 0.375\n",
      "0.3\n",
      "     fun: 0.2944253608584404\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([0.8220237 , 0.2789456 , 0.06892867, ..., 0.5292395 ,\n",
      "             0.4375666 , 0.554462  ], dtype=float32)\n",
      "Test accuracy: 0.375\n",
      "0.4\n",
      "     fun: 0.30258519649505616\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([ 0.7383074 ,  0.365936  , -0.07684992, ...,  0.52489346,\n",
      "              0.45651916,  0.7382485 ], dtype=float32)\n",
      "Test accuracy: 0.375\n",
      "0.5\n",
      "     fun: 0.29949972685426474\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([0.826296  , 0.38750994, 0.02574752, ..., 0.47471374,\n",
      "             0.5005768 , 0.65868306], dtype=float32)\n",
      "Test accuracy: 0.375\n",
      "0.6\n",
      "     fun: 0.2895963290706277\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([0.78822577, 0.2982549 , 0.03964106, ..., 0.5515897 ,\n",
      "             0.47717577, 0.5257237 ], dtype=float32)\n",
      "Test accuracy: 0.375\n",
      "0.7\n",
      "     fun: 0.29470159895718095\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([0.8632347 , 0.20173793, 0.17360036, ..., 0.5407191 ,\n",
      "             0.42048943, 0.75795263], dtype=float32)\n",
      "Test accuracy: 0.375\n",
      "0.8\n",
      "     fun: 0.2734932765364647\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([ 0.8177294 ,  0.30336276, -0.0087424 , ...,  0.54256755,\n",
      "              0.5148607 ,  0.47973052], dtype=float32)\n",
      "Test accuracy: 0.3875\n",
      "0.90000004\n",
      "     fun: 0.28221484515815976\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([0.8809798 , 0.33761978, 0.13755152, ..., 0.5462321 ,\n",
      "             0.26301134, 0.56047964], dtype=float32)\n",
      "Test accuracy: 0.375\n",
      "1.0\n",
      "     fun: 0.28695706166327\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([ 0.64009476,  0.35361996, -0.24911383, ...,  0.54088426,\n",
      "              0.33973202,  0.73397493], dtype=float32)\n",
      "Test accuracy: 0.375\n",
      "1.1\n",
      "     fun: 0.28077284544706343\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([0.9209523 , 0.38662985, 0.11147768, ..., 0.5066509 ,\n",
      "             0.88658786, 0.94284964], dtype=float32)\n",
      "Test accuracy: 0.39375\n",
      "1.2\n",
      "     fun: 0.2835155007895082\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([0.3655618 , 0.60724646, 0.2254794 , ..., 0.41046765,\n",
      "             0.5128469 , 0.65609103], dtype=float32)\n",
      "Test accuracy: 0.36875\n",
      "1.3000001\n",
      "     fun: 0.2887846127152443\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([ 0.82408106, -0.08170094, -0.01227274, ...,  0.5572996 ,\n",
      "              0.41076106,  0.6843331 ], dtype=float32)\n",
      "Test accuracy: 0.39375\n",
      "1.4\n",
      "     fun: 0.26043362310156226\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([0.5758439 , 0.22661072, 0.6578782 , ..., 0.5137101 ,\n",
      "             0.5706093 , 0.678588  ], dtype=float32)\n",
      "Test accuracy: 0.36875\n",
      "1.5\n",
      "     fun: 0.27237118016928435\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([0.36359444, 0.29856977, 0.22172719, ..., 0.7053779 ,\n",
      "             0.6119152 , 0.37733522], dtype=float32)\n",
      "Test accuracy: 0.3625\n",
      "1.6\n",
      "     fun: 0.277254503313452\n",
      " message: 'terminated after reaching max number of iterations'\n",
      "    nfev: 20000\n",
      "     nit: 10000\n",
      " success: True\n",
      "       x: DeviceArray([ 0.8193364 ,  0.29645056, -0.06141445, ...,  0.10984009,\n",
      "              0.37106073,  0.61328447], dtype=float32)\n",
      "Test accuracy: 0.3875\n",
      "1.7\n"
     ]
    }
   ],
   "source": [
    "j = 1\n",
    "for i in np.linspace(0,20,201):\n",
    "    new_a = np.round(i, 1)\n",
    "    print(new_a)\n",
    "    result = minimizeSPSA(train_cost_fn, x0=init_params_spsa, a = new_a, c = 0.2, niter=EPOCHS, callback=dev_cost_fn)\n",
    "    print(result)\n",
    "    \n",
    "    fig, ((ax_tl, ax_tr), (ax_bl, ax_br)) = plt.subplots(2, 2, sharex=True, sharey='row', figsize=(10, 6))\n",
    "    ax_tl.set_title('Training set')\n",
    "    ax_tr.set_title('Development set')\n",
    "    ax_bl.set_xlabel('Iterations')\n",
    "    ax_br.set_xlabel('Iterations')\n",
    "    ax_bl.set_ylabel('Accuracy')\n",
    "    ax_tl.set_ylabel('Loss')\n",
    "\n",
    "    colours = iter(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "    ax_tl.plot(train_costs[1::2], color=next(colours))  # training evaluates twice per iteration\n",
    "    ax_bl.plot(train_accs[1::2], color=next(colours))   # so take every other entry\n",
    "    ax_tr.plot(dev_costs, color=next(colours))\n",
    "    ax_br.plot(dev_accs, color=next(colours))\n",
    "\n",
    "    # print test accuracy\n",
    "    test_cost_fn, _, test_accs = make_cost_fn(test_pred_fn, test_data_labels_l)\n",
    "    test_cost_fn(result.x)\n",
    "    print('Test accuracy:', test_accs[0])\n",
    "    \n",
    "    with open(this_folder + \"//results//result\" + str(j) + \".p\", \"wb\") as f:\n",
    "        pickle.dump(result, f)\n",
    "    with open(this_folder + \"//results//result_stats\" + str(j) + \".p\", \"wb\") as f:\n",
    "        stats = {\"train_costs\":train_costs,\"train_accs\":train_accs,\"dev_costs\":dev_costs,\"dev_accs\":dev_accs, \"a\" : new_a}\n",
    "        pickle.dump(stats, f)\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537aa180-ea99-48b1-90a3-097e4a9ecc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax_tl, ax_tr), (ax_bl, ax_br)) = plt.subplots(2, 2, sharex=True, sharey='row', figsize=(10, 6))\n",
    "ax_tl.set_title('Training set')\n",
    "ax_tr.set_title('Development set')\n",
    "ax_bl.set_xlabel('Iterations')\n",
    "ax_br.set_xlabel('Iterations')\n",
    "ax_bl.set_ylabel('Accuracy')\n",
    "ax_tl.set_ylabel('Loss')\n",
    "\n",
    "colours = iter(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "ax_tl.plot(train_costs[1::2], color=next(colours))  # training evaluates twice per iteration\n",
    "ax_bl.plot(train_accs[1::2], color=next(colours))   # so take every other entry\n",
    "ax_tr.plot(dev_costs, color=next(colours))\n",
    "ax_br.plot(dev_accs, color=next(colours))\n",
    "\n",
    "# print test accuracy\n",
    "test_cost_fn, _, test_accs = make_cost_fn(test_pred_fn, test_data_labels_l)\n",
    "test_cost_fn(result.x)\n",
    "print('Test accuracy:', test_accs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ecdc65-e90a-45dc-973c-074f6c7415e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
