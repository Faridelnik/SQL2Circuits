{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09952d6-99a3-41a5-865f-2170e9ee0247",
   "metadata": {},
   "source": [
    "# Circuit learning module: Lambeq manually with SPSA and JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb811fe4-d729-4a44-838d-9c02db4a644d",
   "metadata": {},
   "source": [
    "This module performs the optimization of the parametrized circuit manually compared to Lambeq's automatic QuantumTrainer class. I created this because I wanted to have more control over the optimization process and debug it better. The code is based on the workflow presented in https://github.com/CQCL/Quanthoven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a5783f-901a-4bfe-a622-4003b345fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "from jax import numpy as np\n",
    "from sympy import default_sort_key\n",
    "import numpy\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "from jax import jit\n",
    "from noisyopt import minimizeSPSA, minimizeCompass\n",
    "\n",
    "from discopy.quantum import Circuit\n",
    "from discopy.tensor import Tensor\n",
    "from discopy.utils import loads\n",
    "#from pytket.extensions.qiskit import AerBackend\n",
    "#from pytket.extensions.qulacs import QulacsBackend\n",
    "#from pytket.extensions.cirq import CirqStateSampleBackend\n",
    "backend = None\n",
    "\n",
    "from utils import *\n",
    "#from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "this_folder = os.path.abspath(os.getcwd())\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "#os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "# This avoids TracerArrayConversionError from jax\n",
    "Tensor.np = np\n",
    "\n",
    "rng = numpy.random.default_rng(SEED)\n",
    "numpy.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238fca40-5d8d-4ace-a65f-693885a8caf8",
   "metadata": {},
   "source": [
    "## Read circuit data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6f00d7-b573-4c5d-a0d6-d5eafc54745e",
   "metadata": {},
   "source": [
    "We read the circuits from the pickled files. Select if we perform binary classification or multi-class classification. Give number of qubits to create classes:\n",
    "- 1 qubits -> 2^1 = 2 classes i.e. binary classification\n",
    "- 2 qubits -> 2^2 = 4 classes\n",
    "- ...\n",
    "- 5 qubits -> 2^5 = 32 classes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01bac1f9-3bc0-4a25-8f4f-baa2b9c581e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select workload\n",
    "#workload = \"execution_time\"\n",
    "workload = \"cardinality\"\n",
    "\n",
    "# Select workload size\n",
    "#workload_size = \"small\"\n",
    "#workload_size = \"medium\"\n",
    "#workload_size = \"large\"\n",
    "workload_size = \"main\"\n",
    "\n",
    "classification = 2\n",
    "layers = 1\n",
    "single_qubit_params = 3\n",
    "n_wire_count = 1\n",
    "\n",
    "loss = multi_class_loss\n",
    "acc = multi_class_acc\n",
    "\n",
    "if classification == 1:\n",
    "    loss = bin_class_loss\n",
    "    acc = bin_class_acc\n",
    "\n",
    "# Access the selected circuits\n",
    "path_name = this_folder + \"//simplified-JOB-diagrams//\" + workload + \"//\" + workload_size + \"//circuits//\" + str(classification) + \"//\" + str(layers) + \"_layer//\" + str(single_qubit_params) + \"_single_qubit_params//\" + str(n_wire_count) + \"_n_wire_count//\"\n",
    "\n",
    "training_circuits_paths = glob.glob(path_name + \"training//[0-9]*.p\")\n",
    "validation_circuits_paths = glob.glob(path_name + \"validation//[0-9]*.p\")\n",
    "test_circuits_paths = glob.glob(path_name + \"test//[0-9]*.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff0a551-444c-40a7-bf66-8e929bc49a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_circuits = read_diagrams(training_circuits_paths)\n",
    "validation_circuits = read_diagrams(validation_circuits_paths)\n",
    "test_circuits = read_diagrams(test_circuits_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da3a7c-6a93-44e9-9640-7e9ead005cb2",
   "metadata": {},
   "source": [
    "## Read training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e9f52-69c9-4e1c-8a70-76e1198d12f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data, validation_data = None, None, None\n",
    "data_path = this_folder + \"//data//\" + workload + \"//\" + workload_size + \"//\"\n",
    "\n",
    "with open(data_path + \"training_data.json\", \"r\") as inputfile:\n",
    "    training_data = json.load(inputfile)['training_data']\n",
    "with open(data_path + \"test_data.json\", \"r\") as inputfile:\n",
    "    test_data = json.load(inputfile)['test_data']\n",
    "with open(data_path + \"validation_data.json\", \"r\") as inputfile:\n",
    "    validation_data = json.load(inputfile)['validation_data']\n",
    "\n",
    "training_data_labels = create_labeled_classes(training_data, classification, workload)\n",
    "test_data_labels = create_labeled_classes(test_data, classification, workload)\n",
    "validation_data_labels = create_labeled_classes(validation_data, classification, workload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572abfc1-cc81-4fdb-b8bb-6813fe658dd6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c65634-fa13-47b3-915c-83361d0e1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred_fn(circuits):\n",
    "    # In the case we want to use other backends. \n",
    "    # Currently does not work properly.\n",
    "    if backend:\n",
    "        compiled_circuits1 = backend.get_compiled_circuits([c.to_tk() for c in circuits])\n",
    "        circuits = [Circuit.from_tk(c) for c in compiled_circuits1]\n",
    "        \n",
    "    circuit_fns = [c.lambdify(*parameters) for c in circuits]\n",
    "    \n",
    "    def predict(params):\n",
    "        outputs = Circuit.eval(*(c(*params) for c in circuit_fns), backend = backend)\n",
    "        res = []\n",
    "        \n",
    "        for output in outputs:\n",
    "            predictions = np.abs(output.array) + 1e-9\n",
    "            ratio = predictions / predictions.sum()\n",
    "            res.append(ratio)\n",
    "            \n",
    "        return np.array(res)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cf7e7-fcbf-4bad-b8bc-4ff0e6b29294",
   "metadata": {},
   "source": [
    "## Loss function and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376b4aa-0ec6-4037-80eb-935aa75a2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cost_fn(pred_fn, labels):\n",
    "    def cost_fn(params, **kwargs):\n",
    "        predictions = pred_fn(params)\n",
    "        #print(predictions, labels)\n",
    "        \n",
    "        cost = loss(predictions, labels) #-np.sum(labels * np.log(predictions)) / len(labels)  # binary cross-entropy loss\n",
    "        costs.append(cost)\n",
    "\n",
    "        accuracy = acc(predictions, labels) #np.sum(np.round(predictions) == labels) / len(labels) / 2  # half due to double-counting\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        return cost\n",
    "\n",
    "    costs, accuracies = [], []\n",
    "    return cost_fn, costs, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bb6805-facb-4ec9-9c38-8d1df42022b2",
   "metadata": {},
   "source": [
    "## Minimization with noisyopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b70bae-fe88-4852-a423-658641380e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(old_params, old_values, new_params):\n",
    "    new_values = list(numpy.array(rng.random(len(new_params))))\n",
    "    old_param_dict = {}\n",
    "    for p, v in zip(old_params, old_values):\n",
    "        old_param_dict[p] = v\n",
    "        \n",
    "    parameters = sorted(set(old_params + new_params), key=default_sort_key)\n",
    "    values = []\n",
    "    for p in parameters:\n",
    "        if p in old_param_dict:\n",
    "            values.append(old_param_dict[p])\n",
    "        else:\n",
    "            values.append(new_values.pop())\n",
    "            \n",
    "    return parameters, np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd1d87d-047d-4feb-9dbc-c972bbabf741",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "initial_number_of_circuits = 10\n",
    "syms = {}\n",
    "limit = False\n",
    "all_training_keys = list(training_circuits.keys())\n",
    "initial_circuit_keys = all_training_keys[:initial_number_of_circuits + 1]\n",
    "current_training_circuits = {}\n",
    "result_file = workload + \"_\" + workload_size + \"_noisyopt_test_\" + str(classification) + \"_\" + str(layers) + \"_\" + str(single_qubit_params)\n",
    "\n",
    "for k in initial_circuit_keys:\n",
    "    current_training_circuits[k] = training_circuits[k]\n",
    "    \n",
    "syms = get_symbols(current_training_circuits)\n",
    "parameters = sorted(syms, key=default_sort_key)\n",
    "if initial_number_of_circuits > 10 and os.path.exists(\"points//\" + result_file + \".npz\"):\n",
    "    with open(\"points//\" + result_file + \".npz\", \"rb\") as f:\n",
    "        print(\"Loading parameters from file \" + result_file)\n",
    "        npzfile = np.load(f)\n",
    "        init_params_spsa = npzfile['arr_0']\n",
    "else:\n",
    "    print(\"Initializing new parameters\")\n",
    "    init_params_spsa = np.array(rng.random(len(parameters)))\n",
    "result = None\n",
    "run = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d19610-d096-4de1-b1b9-63aef886e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = None\n",
    "\n",
    "for i, key in enumerate(all_training_keys[initial_number_of_circuits:]):\n",
    "    print(\"Progress: \", round((i + initial_number_of_circuits)/len(all_training_keys), 3))\n",
    "    \n",
    "    if len(syms) == len(get_symbols(current_training_circuits)) and i > 0:\n",
    "        if i != len(all_training_keys[1:]):\n",
    "            current_training_circuits[key] = training_circuits[key]\n",
    "            new_parameters = sorted(get_symbols({key: training_circuits[key]}), key=default_sort_key)\n",
    "            if result:\n",
    "                parameters, init_params_spsa = initialize_parameters(parameters, result.x, new_parameters)\n",
    "                #continue\n",
    "            else:\n",
    "                syms = get_symbols(current_training_circuits)\n",
    "                parameters = sorted(syms, key=default_sort_key)\n",
    "                init_params_spsa = np.array(rng.random(len(parameters)))\n",
    "    else:\n",
    "        run += 1\n",
    "    \n",
    "    # Select those circuits from test and validation circuits which share the parameters with the current training circuits\n",
    "    current_validation_circuits = select_circuits(current_training_circuits, validation_circuits)\n",
    "    current_test_circuits = select_circuits(current_training_circuits, test_circuits)\n",
    "    \n",
    "    if len(current_validation_circuits) == 0 or len(current_test_circuits) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Create lists with circuits and their corresponding label\n",
    "    training_circuits_l, training_data_labels_l = construct_data_and_labels(current_training_circuits, training_data_labels)\n",
    "    validation_circuits_l, validation_data_labels_l = construct_data_and_labels(current_validation_circuits, validation_data_labels)\n",
    "    test_circuits_l, test_data_labels_l = construct_data_and_labels(current_test_circuits, test_data_labels)\n",
    "    \n",
    "    # Limit the number of validation and test circuits to 20% of number of the training circuits\n",
    "    if limit:\n",
    "        val_test_circ_size = ceil(len(current_training_circuits))\n",
    "        if len(current_validation_circuits) > val_test_circ_size:\n",
    "            validation_circuits_l = validation_circuits_l[:val_test_circ_size]\n",
    "            validation_data_labels_l = validation_data_labels_l[:val_test_circ_size]\n",
    "        if len(current_test_circuits) > val_test_circ_size:\n",
    "            test_circuits_l = test_circuits_l[:val_test_circ_size]\n",
    "            test_data_labels_l = test_data_labels_l[:val_test_circ_size]\n",
    "    \n",
    "    stats = f\"Number of training circuits: {len(training_circuits_l)}   \"\\\n",
    "        + f\"Number of validation circuits: {len(validation_circuits_l)}   \"\\\n",
    "        + f\"Number of test circuits: {len(test_circuits_l)}   \"\\\n",
    "        + f\"Number of parameters in model: {len(set([sym for circuit in training_circuits_l for sym in circuit.free_symbols]))}\"\n",
    "    \n",
    "    with open(\"results//\" + result_file + \".txt\", \"a\") as f:\n",
    "        f.write(stats + \"\\n\")\n",
    "    \n",
    "    print(stats)\n",
    "    \n",
    "    optimization_interval = 20\n",
    "    \n",
    "    if result == None or run % optimization_interval == 0:\n",
    "    \n",
    "        train_pred_fn = jit(make_pred_fn(training_circuits_l))\n",
    "        dev_pred_fn = jit(make_pred_fn(validation_circuits_l))\n",
    "        test_pred_fn = make_pred_fn(test_circuits_l)\n",
    "\n",
    "        train_cost_fn, train_costs, train_accs = make_cost_fn(train_pred_fn, training_data_labels_l)\n",
    "        dev_cost_fn, dev_costs, dev_accs = make_cost_fn(dev_pred_fn, validation_data_labels_l)\n",
    "\n",
    "        def callback_fn(xk):\n",
    "            #print(xk)\n",
    "            valid_loss = dev_cost_fn(xk)\n",
    "            train_loss = numpy.around(min(float(train_costs[-1]), float(train_costs[-2])), 4)\n",
    "            train_acc = numpy.around(min(float(train_accs[-1]), float(train_accs[-2])), 4)\n",
    "            valid_acc = numpy.around(float(dev_accs[-1]), 4)\n",
    "            iters = int(len(train_accs)/2)\n",
    "            if iters % 200 == 0:\n",
    "                info = f\"Epoch: {iters}   \"\\\n",
    "                + f\"train/loss: {train_loss}   \"\\\n",
    "                + f\"valid/loss: {numpy.around(float(valid_loss), 4)}   \"\\\n",
    "                + f\"train/acc: {train_acc}   \"\\\n",
    "                + f\"valid/acc: {valid_acc}\"\n",
    "\n",
    "                with open(\"results//\" + result_file + \".txt\", \"a\") as f:\n",
    "                    f.write(info + \"\\n\")\n",
    "\n",
    "                print(info, file=sys.stderr)\n",
    "            return valid_loss\n",
    "\n",
    "        #a_value = 0.0053\n",
    "        #c_value = 0.0185\n",
    "\n",
    "        # Good\n",
    "        #a_value = 0.053\n",
    "        #c_value = 0.00185\n",
    "\n",
    "        #a_value = 0.0053\n",
    "        #c_value = 0.00185\n",
    "        \n",
    "        a_values = [0.1, 0.01, 0.001, 0.0001]\n",
    "        c_values = [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "        \n",
    "        train_cost_fn, train_costs, train_accs = make_cost_fn(train_pred_fn, training_data_labels_l)\n",
    "        dev_cost_fn, dev_costs, dev_accs = make_cost_fn(dev_pred_fn, validation_data_labels_l)\n",
    "        \n",
    "        for a_value in a_values:\n",
    "            for c_value in c_values:\n",
    "                print(a_value, c_value)\n",
    "                result = minimizeSPSA(train_cost_fn, x0=init_params_spsa, a = a_value, c = c_value, niter=EPOCHS, callback=callback_fn)\n",
    "                #result = minimizeCompass(train_cost_fn, x0=init_params_spsa, redfactor=2.0, deltainit=1.0, deltatol=0.001, feps=1e-15, errorcontrol=True, funcNinit=30, funcmultfactor=2.0, paired=True, alpha=0.05, callback=callback_fn)\n",
    "\n",
    "        #figure_path = this_folder + \"//results//\" + result_file + \".png\"\n",
    "        #visualize_result_noisyopt(result, make_cost_fn, test_pred_fn, test_data_labels_l, train_costs, train_accs, dev_costs, dev_accs, figure_path, result_file)\n",
    "    \n",
    "    run += 1\n",
    "    #EPOCHS += 100\n",
    "    syms = get_symbols(current_training_circuits)\n",
    "    \n",
    "    # Extend for the next optimization round\n",
    "    current_training_circuits[key] = training_circuits[key]\n",
    "    new_parameters = sorted(get_symbols({key: training_circuits[key]}), key=default_sort_key)\n",
    "    parameters, init_params_spsa = initialize_parameters(parameters, result.x, new_parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
