{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09952d6-99a3-41a5-865f-2170e9ee0247",
   "metadata": {},
   "source": [
    "# Circuit learning module: Lambeq's QuantumTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb811fe4-d729-4a44-838d-9c02db4a644d",
   "metadata": {},
   "source": [
    "This module performs the optimization with Lambeq's native optimizer. Because the circuits are constructed with Lambeq and DisCoPy, this optimizer is the natural choice. The code is based on the workflow presented in https://github.com/CQCL/lambeq/blob/main/docs/examples/quantum_pipeline.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a5783f-901a-4bfe-a622-4003b345fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from discopy.utils import loads\n",
    "from pytket.extensions.qiskit import AerBackend\n",
    "from lambeq import TketModel\n",
    "from lambeq import QuantumTrainer, SPSAOptimizer\n",
    "from lambeq import Dataset\n",
    "\n",
    "this_folder = os.path.abspath(os.getcwd())\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1000\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238fca40-5d8d-4ace-a65f-693885a8caf8",
   "metadata": {},
   "source": [
    "## Read circuit data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6f00d7-b573-4c5d-a0d6-d5eafc54745e",
   "metadata": {},
   "source": [
    "We read the circuits from the pickled files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff0a551-444c-40a7-bf66-8e929bc49a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training query:  10b\n",
      "training query:  10c\n",
      "training query:  10d\n",
      "training query:  11b\n",
      "training query:  11c\n",
      "training query:  1b\n",
      "training query:  1c\n",
      "training query:  2b\n",
      "training query:  2c\n",
      "training query:  2d\n",
      "training query:  5b\n",
      "training query:  5c\n",
      "training query:  5d\n",
      "training query:  6b\n",
      "training query:  6c\n",
      "training query:  6d\n",
      "training query:  7b\n",
      "training query:  7c\n",
      "training query:  7d\n",
      "training query:  7e\n",
      "training query:  8b\n",
      "training query:  9b\n",
      "training query:  9c\n",
      "training query:  9d\n",
      "training query:  9e\n",
      "test query:  10a\n",
      "test query:  11a\n",
      "test query:  1a\n",
      "test query:  2a\n",
      "test query:  4a\n",
      "test query:  5a\n",
      "test query:  6a\n",
      "test query:  7a\n",
      "test query:  8a\n",
      "test query:  9a\n"
     ]
    }
   ],
   "source": [
    "training_circuits_paths = glob.glob(this_folder + \"//simplified-JOB-diagrams//circuits//binary_classification//training//[0-9]*.p\")\n",
    "#validation_circuits_paths = glob.glob(this_folder + \"//simplified-JOB-diagrams//circuits//binary_classification//validation//[0-9]*.p\")\n",
    "test_circuits_paths = glob.glob(this_folder + \"//simplified-JOB-diagrams//circuits//binary_classification//test//[0-9]*.p\")\n",
    "\n",
    "def read_diagrams(circuit_paths):\n",
    "    circuits = {}\n",
    "    for serialized_diagram in circuit_paths:\n",
    "        base_name = Path(serialized_diagram).stem\n",
    "        f = open(serialized_diagram, \"rb\")\n",
    "        diagram = pickle.load(f)\n",
    "        circuits[base_name] = diagram\n",
    "    return circuits\n",
    "\n",
    "\n",
    "training_circuits = read_diagrams(training_circuits_paths) #+ training_circuits_paths[21:])\n",
    "for key in training_circuits:\n",
    "    print(\"training query: \", key)\n",
    "test_circuits = read_diagrams(test_circuits_paths) #+ [test_circuits_paths[2]] + test_circuits_paths[8:])\n",
    "#test_circuits = read_diagrams([test_circuits_paths[2]])\n",
    "for key in test_circuits:\n",
    "    print(\"test query: \", key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da3a7c-6a93-44e9-9640-7e9ead005cb2",
   "metadata": {},
   "source": [
    "## Read training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "774e9f52-69c9-4e1c-8a70-76e1198d12f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = None, None\n",
    "with open(this_folder + \"//data//training_data.json\", \"r\") as inputfile:\n",
    "    training_data = json.load(inputfile)['training_data']\n",
    "with open(this_folder + \"//data//test_data.json\", \"r\") as inputfile:\n",
    "    test_data = json.load(inputfile)['test_data']\n",
    "    \n",
    "\n",
    "def time_to_states(data):\n",
    "    labeled_data = {}\n",
    "    for elem in data:\n",
    "        if elem[\"name\"] in training_circuits.keys() or elem[\"name\"] in test_circuits.keys():\n",
    "            if elem[\"time\"] < 2001:\n",
    "                labeled_data[elem[\"name\"]] = [1,0] # corresponds to |0>\n",
    "            else:\n",
    "                labeled_data[elem[\"name\"]] = [0,1] # corresponds to |1>\n",
    "    return labeled_data\n",
    "\n",
    "\n",
    "training_data_labels = time_to_states(training_data)\n",
    "test_data_labels = time_to_states(test_data)\n",
    "\n",
    "#for key in training_data_labels:\n",
    "#    print(\"training: \", key)\n",
    "#for key in test_data_labels:\n",
    "#    print(\"test \", key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cad830-e6c3-4c29-9ce6-0d6bec2794bc",
   "metadata": {},
   "source": [
    "## Lambeq optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce559dbf-2b4b-4853-82ed-58832aa11e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test circuits need to share training circuits' parameters. The parameters that are not covered:  {'D%'__n.l_0, 'D%'__n.l_2, 'features'__n.l_0, company_id__n.l_2, name_pcode_cf__n.l_1, FROM_n@n.l@n.l@n.l@n.l@n.l_n_3, name_pcode_cf__n.l_0, company_name__n.l_0, FROM_n@n.l@n.l@n.l@n.l@n.l_n_0, company_id__n.l_0, FROM_n@n.l@n.l@n.l@n.l@n.l_n_4, 'features'__n.l_1, 'features'__n.l_2, name_pcode_cf__n.l_2, FROM_n@n.l@n.l@n.l@n.l@n.l_n_2, FROM_n@n.l@n.l@n.l@n.l@n.l_n_1, company_name__n.l_2, company_id__n.l_1, 'D%'__n.l_1, company_name__n.l_1}\n",
      "Total number of circuits:  35\n",
      "Total number of variables:  1158\n"
     ]
    }
   ],
   "source": [
    "#all_circuits = list(training_circuits.values()) + list(test_circuits.values())\n",
    "\n",
    "training_circuits_l = []\n",
    "test_circuits_l = []\n",
    "training_data_labels_l = []\n",
    "test_data_labels_l = []\n",
    "\n",
    "for key in training_circuits:\n",
    "    training_circuits_l.append(training_circuits[key])\n",
    "    training_data_labels_l.append(training_data_labels[key])\n",
    "\n",
    "for key in test_circuits:\n",
    "    test_circuits_l.append(test_circuits[key])\n",
    "    test_data_labels_l.append(test_data_labels[key])\n",
    "\n",
    "all_circuits = training_circuits_l + test_circuits_l\n",
    "\n",
    "train_syms = set([sym for circuit in training_circuits.values() for sym in circuit.free_symbols])\n",
    "test_syms = set([sym for circuit in test_circuits.values() for sym in circuit.free_symbols])\n",
    "\n",
    "print(\"Test circuits need to share training circuits' parameters. The parameters that are not covered: \", test_syms.difference(train_syms))\n",
    "\n",
    "print(\"Total number of circuits: \", len(all_circuits))\n",
    "print(\"Total number of variables: \", len([sym for circuit in all_circuits for sym in circuit.free_symbols]))\n",
    "\n",
    "backend = AerBackend()\n",
    "backend_config = {\n",
    "    'backend': backend,\n",
    "    'compilation': backend.default_compilation_pass(2),\n",
    "    'shots': 32768\n",
    "}\n",
    "\n",
    "model = TketModel.from_diagrams(all_circuits, backend_config=backend_config)\n",
    "model.initialise_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cf7e7-fcbf-4bad-b8bc-4ff0e6b29294",
   "metadata": {},
   "source": [
    "## Loss function and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cde594c-ad61-4dd6-ac79-85496f2cfed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lambda y_hat, y: -np.sum(y * np.log(y_hat)) / len(y)  # binary cross-entropy loss\n",
    "#acc = lambda y_hat, y: np.sum(np.round(y_hat) == y) / len(y) / 2  # half due to double-counting\n",
    "\n",
    "def acc(y_hat, y):\n",
    "    #print(\"y_hat: \", y_hat)\n",
    "    #print(\"y: \", y)\n",
    "    return (np.sum(np.round(y_hat) == y) / len(y)) / 2\n",
    "    \n",
    "eval_metrics = {\"acc\": acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bb6805-facb-4ec9-9c38-8d1df42022b2",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47dbc0e6-4261-498f-92fc-f71e28a8aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QuantumTrainer(\n",
    "    model,\n",
    "    loss_function=loss,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=SPSAOptimizer,\n",
    "    optim_hyperparams={'a': 0.2, 'c': 0.06, 'A':0.01*EPOCHS},\n",
    "    evaluate_functions=eval_metrics,\n",
    "    evaluate_on_train=True,\n",
    "    verbose = 'text',\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a9dfb7-5f6d-4128-867b-54e9c4e21f0b",
   "metadata": {},
   "source": [
    "## Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a37c01-0d77-49d7-a013-feae68278c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:     train/loss: 1.9808   valid/loss: -----   train/acc: 0.5200   valid/acc: -----\n",
      "Epoch 2:     train/loss: 0.6573   valid/loss: -----   train/acc: 0.5000   valid/acc: -----\n",
      "Epoch 3:     train/loss: 1.1959   valid/loss: -----   train/acc: 0.4800   valid/acc: -----\n",
      "Epoch 4:     train/loss: 0.8978   valid/loss: -----   train/acc: 0.5800   valid/acc: -----\n",
      "Epoch 5:     train/loss: 2.0407   valid/loss: -----   train/acc: 0.4600   valid/acc: -----\n",
      "Epoch 6:     train/loss: 1.7708   valid/loss: -----   train/acc: 0.4400   valid/acc: -----\n",
      "Epoch 7:     train/loss: 1.5953   valid/loss: -----   train/acc: 0.4800   valid/acc: -----\n",
      "Epoch 8:     train/loss: 1.7408   valid/loss: -----   train/acc: 0.5000   valid/acc: -----\n",
      "Epoch 9:     train/loss: 1.4804   valid/loss: -----   train/acc: 0.5000   valid/acc: -----\n",
      "Epoch 10:    train/loss: 1.4481   valid/loss: -----   train/acc: 0.5400   valid/acc: -----\n",
      "Epoch 11:    train/loss: 1.5323   valid/loss: -----   train/acc: 0.4600   valid/acc: -----\n",
      "Epoch 12:    train/loss: 1.7165   valid/loss: -----   train/acc: 0.5600   valid/acc: -----\n",
      "Epoch 13:    train/loss: 2.9430   valid/loss: -----   train/acc: 0.3400   valid/acc: -----\n",
      "Epoch 14:    train/loss: 1.6716   valid/loss: -----   train/acc: 0.5200   valid/acc: -----\n",
      "Epoch 15:    train/loss: 3.2652   valid/loss: -----   train/acc: 0.5400   valid/acc: -----\n",
      "Epoch 16:    train/loss: 1.7815   valid/loss: -----   train/acc: 0.4400   valid/acc: -----\n",
      "Epoch 17:    train/loss: 1.1627   valid/loss: -----   train/acc: 0.4800   valid/acc: -----\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(\n",
    "            training_circuits_l,\n",
    "            training_data_labels_l,\n",
    "            batch_size=BATCH_SIZE)\n",
    "\n",
    "#test_dataset = Dataset(test_circuits_l, test_data_labels_l, shuffle=False)\n",
    "\n",
    "trainer.fit(train_dataset, evaluation_step=1, logging_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8038a05-dc8e-4d56-a331-5cfde775941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ((ax_tl, ax_tr), (ax_bl, ax_br)) = plt.subplots(2, 2, sharex=True, sharey='row', figsize=(10, 6))\n",
    "ax_tl.set_title('Training set')\n",
    "ax_tr.set_title('Development set')\n",
    "ax_bl.set_xlabel('Iterations')\n",
    "ax_br.set_xlabel('Iterations')\n",
    "ax_bl.set_ylabel('Accuracy')\n",
    "ax_tl.set_ylabel('Loss')\n",
    "\n",
    "colours = iter(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "ax_tl.plot(trainer.train_epoch_costs[::10], color=next(colours))\n",
    "ax_bl.plot(trainer.train_results['acc'][::10], color=next(colours))\n",
    "ax_tr.plot(trainer.val_costs[::10], color=next(colours))\n",
    "ax_br.plot(trainer.val_results['acc'][::10], color=next(colours))\n",
    "\n",
    "# print test accuracy\n",
    "test_acc = acc(model(test_circuits_l), test_data_labels_l)\n",
    "print('Validation accuracy:', test_acc.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
